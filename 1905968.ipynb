{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 川普, 来, 跟爷一起说  WeChaty + PaddleHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一、项目背景\n",
    "\n",
    "本项目有趣的地方在于可以使用自己说话的语音信息和其他人的照片, 逼真地生成一幅他人模仿你说话的影像。其原理是根据音频直接合成唇形动作。语音信息通过wechaty终端传入，再基于PaddleGAN的Wav2Lip模型直接端到端的生成。本项目以特朗普为例，我们说啥就让特朗普跟着说啥，不仅支持语音信息, 中英文本，还可以传入带有文字的图片。 功能上不仅能增加一定的趣味，给用户一种新的AI体验， 同时用户能基于此，扩展到其他应用，比如结合翻译模型识别图片上的外文，生成有声小说，创建点读笔应用，躺着听论文等等。\n",
    "\n",
    "本项目是参加【**AI ChatBot 创意赛**】的作品，PaddleHub携手开源聊天机器人框架WeChaty带来 AI ChatBot创意赛，为AI算法工程师提供一个全新的应用场景：Chatbot （Conversational AI），同时也为Chatbot 开发者提供一个全新的AI能力平台，拓宽视野，为未来设计更加强大的 Chatbot 提供一扇门。\n",
    "\n",
    "本项目主要难点是一方面要融合OCR， GAN， Parakeet三大模块，在兼容部署环境上和代码集成上有一定的挑战， 另一方面PaddlePaddle目前没有开源中文语音合成的模型，目前只能使用ZHTTS临时方案，后期若有时间可将此模型迁移至PaddlePaddle。\n",
    "\n",
    "关于Wechaty的安装和使用请参考我另一篇： [微信医聊自动问答 WeChaty + PaddleHub ](https://aistudio.baidu.com/aistudio/projectdetail/1868162)\n",
    "\n",
    "参考项目：\n",
    "\n",
    "[Parakeet：飞桨，你是个成熟的框架了，要学会自己读论文](https://aistudio.baidu.com/aistudio/projectdetail/676162)\n",
    "\n",
    "[基于PaddleGAN快速让你的照片动起来](http://https://aistudio.baidu.com/aistudio/projectdetail/1773529) \n",
    "\n",
    "由于B站多次上传不成功，请在左侧下在trump.mp4本地观看。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 二、关于WeChaty和PaddleHub\r\n",
    "\r\n",
    "wechaty（https://github.com/wechaty/wechaty）是一款开源的微信SDK，它基于微信公开的API，对接口进行了一系列的封装，提供一系列简单的接口，然后开发者可以在其之上进行微信机器人的开发。\r\n",
    "\r\n",
    "PaddleHub 是基于 PaddlePaddle 开发的预训练模型管理工具，可以借助预训练模型更便捷地开展迁移学习工作，旨在让 PaddlePaddle 生态下的开发者更便捷体验到大规模预训练模型的价值。\r\n",
    "\r\n",
    "关于二者的结合使用请参考我的另一个公开项目：\r\n",
    "\t[微信医聊自动问答 WeChaty + PaddleHub](https://aistudio.baidu.com/aistudio/projectdetail/1868162)\r\n",
    "    \r\n",
    "这里使用docker脚本可方便快速部署。请将your_token处替换成你的WeChaty token (**puppet_padlocal_xxxxxxxxxxxxx)**\r\n",
    "\r\n",
    "```\r\n",
    "export WECHATY_LOG=\"verbose\"\r\n",
    "export WECHATY_PUPPET=\"wechaty-puppet-padlocal\"\r\n",
    "export WECHATY_PUPPET_PADLOCAL_TOKEN=\"your_token\"\r\n",
    "\r\n",
    "export WECHATY_PUPPET_SERVER_PORT=\"8080\"\r\n",
    "export WECHATY_TOKEN=\"your_token\"\r\n",
    "\r\n",
    "docker run -ti \\\r\n",
    "  --name wechaty_puppet_service_token_gateway \\\r\n",
    "  --rm \\\r\n",
    "  -e WECHATY_LOG \\\r\n",
    "  -e WECHATY_PUPPET \\\r\n",
    "  -e WECHATY_PUPPET_PADLOCAL_TOKEN \\\r\n",
    "  -e WECHATY_PUPPET_SERVER_PORT \\\r\n",
    "  -e WECHATY_TOKEN \\\r\n",
    "  -p \"$WECHATY_PUPPET_SERVER_PORT:$WECHATY_PUPPET_SERVER_PORT\" \\\r\n",
    "  wechaty/wechaty:latest\r\n",
    "~                        \r\n",
    "```\r\n",
    "\r\n",
    "运行成功后如下图:\r\n",
    "\r\n",
    "\r\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/73f123bb9b37410eb49f834dc512226e014f3f53a77d42888b201833e8ef6942)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 三、安装环境以及python的依赖包\n",
    "\n",
    "\n",
    "`!pip install -r PaddleGAN/requirements.txt imageio-ffmpeg`\n",
    "\n",
    "`!pip install -U paddlehub`\n",
    "\n",
    "`!pip install ruamel  -U paddle-parakeet`\n",
    "\n",
    "`!pip install shapely pyclipper -i https://pypi.tuna.tsinghua.edu.cn/simple `\n",
    "\n",
    "为了将微信端slk格式的语音文件转为mp3, 需要用到silk-v3-decoder第三方库, 使用的命令如下:\n",
    "\n",
    "`sh converter.sh 33921FF3774A773BB193B6FD4AD7C33E.slk mp3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!git clone https://gitee.com/paddlepaddle/PaddleGAN\r\n",
    "!pip install -r PaddleGAN/requirements.txt imageio-ffmpeg moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'silk-v3-decoder'...\n",
      "remote: Enumerating objects: 697, done.\u001b[K\n",
      "remote: Counting objects: 100% (697/697), done.\u001b[K\n",
      "remote: Compressing objects: 100% (306/306), done.\u001b[K\n",
      "remote: Total 697 (delta 388), reused 697 (delta 388), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (697/697), 75.21 MiB | 523.00 KiB/s, done.\n",
      "Resolving deltas: 100% (388/388), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "#安装silk-v3-decoder\r\n",
    "!git clone https://gitee.com/xiangjilexiaochou/silk-v3-decoder.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 四、Wav2Lip模型介绍\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/adb83d8b854246e0b713df86fa6fb6d4a714a01f01bb42d8a5ef50744ca7c13a\" width='800' height=''></center>\n",
    "<center><br>Wav2Lip唇形同步示意图</br></center>\n",
    "\n",
    "* 在训练阶段，生成器模型输入包含两部分（视频帧序列和音频），分别通过Face encoder和Audio encoder得到特征信息，并进行融合，再通过Face decoder获得唇形和音频同步的图像帧。把原始视频帧和生成图像帧输入到视觉质量判别器中，二分类结果表示是真实的图像、还是生成的图片，进而提高图像质量。把生成图像帧和音频输入到预先训练好的唇形同步判别器中，判断唇形是否生成的精准，在训练过程中，唇形同步判别器参数会一直被冻结，不参与训练、更新。\n",
    "\n",
    "* 在推理阶段，提供一段音频和视频(或图像、动画)即可合成唇形同步视频。\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/7e21a14b990745b7a5f4ce6eedbd70b99204d4e0010647f782f411ce7697b768' width='800' height=''></center>\n",
    "\n",
    "<center><br>Wav2Lip唇形同步设计方案</br></center>\n",
    "\n",
    "av2Lip实现唇形与语音精准同步突破的关键在于，它采用了唇形同步判别器，以强制生成器持续产生准确而逼真的唇部运动。此外，它通过在鉴别器中使用多个连续帧而不是单个帧，并使用视觉质量损失（而不仅仅是对比损失）来考虑时间相关性，从而改善了视觉质量。Wav2Lip适用于任何人脸、任何语言，对任意视频都能达到很高都准确率，可以无缝地与原始视频融合，还可以用于转换动画人脸。\n",
    "\n",
    "使用一个强有力的嘴唇同步鉴别器迫使发生器产生精确的嘴唇形状。然而，它有时会导致变形区域稍微模糊或包含轻微的伪像。为了减轻这种微小的质量损失，我们在 GAN 设置中训练了一个简单的视觉质量鉴别器和生成器。因此，我们有两个鉴别器，一个用于同步精度，另一个用于更好的视觉质量。一方面，唇同步鉴别器不需要进一步微调，即训练过程权重被冻结。另一方面，由于视觉质量鉴别器不对嘴唇同步进行任何检查，并且只惩罚不现实的人脸生成，所以它是在生成的人脸上进行训练的。\n",
    "\n",
    "Wav2Lip模型结构如下图所示：\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/3fbd93a0d6c04b66bfd1ac2f958635e80d0c7cc883e54a9c888e867d564407af' width='800' height=''></center>\n",
    "<center><br>图6：Wav2Lip结构图</br><center>  \n",
    "\n",
    "图6中下半部分被遮挡的目标人脸作为姿态先验输入，这是至关重要的，因为它允许生成的人脸区域无缝地粘回原始视频，不需要进一步的后期处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 五、将Wechaty接收到的语音格式转为mp3\n",
    "\n",
    "wav2lip代码无法直接读取slk格式文件，所以需要先借用工具转换一下。\n",
    "\n",
    "wechaty没有提供发送语音的接口，如果将slk或者mp3发回去，只能是以文件的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd silk-v3-decoder\r\n",
    "!sh converter.sh ../message-7116130031518557320-audio.slk mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 六、将Wechaty接收到的文本信息转为语音\n",
    "\n",
    "**Parakeet模型库介绍**\n",
    "\n",
    "Parakeet是飞桨近期上新的语音合成套件，用于实现端到端的语音合成。如果您使用过各类读书app或者某些浏览器、插件的朗读功能，这些都是典型的TTS（Text To Speech）场景。本项目将使用WaveFlow语音合成模型完成相关任务，并结合Transformer TTS验证语音合成效果，比如输入“Hello World”，文字转语音效果如下：\n",
    "\n",
    "**WaveFlow模型**\n",
    "WaveFlow来自百度研究院的论文WaveFlow: A Compact Flow-based Model for Raw Audio，飞桨复现了该语音合成模型。根据官网介绍，只有5.9M参数量，比经典的WaveGlow语音合成模型小了15倍，同时语音合成效果也非常好。WaveFlow和WaveGlow都是基于流的生成模型，它和GAN都属于生成模型家族。\n",
    "\n",
    "需要注意的是，WaveFlow是个vocoder（声码器，一种将声学参数转换成语音波形的工具）,不能直接实现文字转语音，需要与Parakeet库中的TTS模型Deep Voice 3、Transformer TTS或FastSpeech模型结合，实现文字转语音的拟声合成。\n",
    "\n",
    "**Transformer TTS文字转语音模型**\n",
    "Parakeet使用PaddlePaddle动态图复现了Transformer TTS, 根据论文Neural Speech Synthesis with Transformer Network实现了基于Transformer的语音合成系统。\n",
    "\n",
    "学习资料:\n",
    "\n",
    "[WaveFlow论文地址](http://arxiv.org/abs/1912.01219)\n",
    "\n",
    "[WaveFlow: A Compact Flow-Based Model for Raw Audio](http://research.baidu.com/Blog/index-view?id=139)\n",
    "\n",
    "[Transformer TTS论文地址](http://arxiv.org/abs/1809.08895)\n",
    "\n",
    "[基于Transformer的语音合成系统](http://https://zhuanlan.zhihu.com/p/66931179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#下载预训练权重到pretrained_models\r\n",
    "# !wget https://paddlespeech.bj.bcebos.com/Parakeet/transformer_tts_ljspeech_ckpt_0.2.zip\r\n",
    "# !wget https://paddlespeech.bj.bcebos.com/Parakeet/waveflow_res128_ljspeech_ckpt_0.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#运行时需要先安装nltk库\r\n",
    "import nltk\r\n",
    "nltk.download(\"punkt\")\r\n",
    "nltk.download(\"cmudict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint] Rank 0: loaded model from pretrained_models/transformer_tts_ljspeech_ckpt_0.2/step-310000.pdparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.FP64, but right dtype is VarType.FP32, the right dtype will convert to VarType.FP64\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.FP32, but right dtype is VarType.FP64, the right dtype will convert to VarType.FP32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "  1%|          | 4/501 [00:00<00:13, 36.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint] Rank 0: loaded model from pretrained_models/waveflow_ljspeech_ckpt_0.2/step-2000000.pdparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/501 [00:00<00:13, 36.54it/s] 75%|███████▍  | 374/501 [00:35<00:11, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits stop condition.\n",
      "177.05754160881042\n"
     ]
    }
   ],
   "source": [
    "#这里需要用到config文件, 可以从github里直接拷贝过来\r\n",
    "import parakeet\r\n",
    "from parakeet.frontend import English\r\n",
    "from parakeet.models import TransformerTTS\r\n",
    "from pathlib import Path\r\n",
    "import yacs\r\n",
    "from tts_config import get_cfg_defaults\r\n",
    "from wavflow_config import get_cfg_defaults as wav_get_cfg_defaults\r\n",
    "import soundfile as sf\r\n",
    "from parakeet.models import ConditionalWaveFlow, WaveFlow\r\n",
    "import time\r\n",
    "\r\n",
    "#tts模型\r\n",
    "frontend = English()\r\n",
    "checkpoint_dir = Path(\"pretrained_models/transformer_tts_ljspeech_ckpt_0.2\")\r\n",
    "checkpoint_path = str(checkpoint_dir / \"step-310000\")\r\n",
    "tconfig = get_cfg_defaults()\r\n",
    "tconfig.merge_from_file(str(checkpoint_dir / \"config.yaml\"))\r\n",
    "model = TransformerTTS.from_pretrained(\r\n",
    "    frontend, tconfig, checkpoint_path)\r\n",
    "model.eval()\r\n",
    "\r\n",
    "#waveflow模型\r\n",
    "w_checkpoint_dir = Path(\"pretrained_models/waveflow_ljspeech_ckpt_0.2\")\r\n",
    "w_checkpoint_path = str(w_checkpoint_dir / \"step-2000000\")\r\n",
    "wconfig = wav_get_cfg_defaults()\r\n",
    "wconfig.merge_from_file(str(w_checkpoint_dir / \"config.yaml\"))\r\n",
    "vocoder = ConditionalWaveFlow.from_pretrained(wconfig, w_checkpoint_path)\r\n",
    "vocoder.eval()\r\n",
    "\r\n",
    "\r\n",
    "def infer(txt, out_path):\r\n",
    "    stime = time.time()\r\n",
    "    outputs = model.predict(sentence)\r\n",
    "    mel_output = outputs[\"mel_output\"]\r\n",
    "    audio = vocoder.predict(mel_output.T)\r\n",
    "    sf.write(out_path, audio, wconfig.data.sample_rate)\r\n",
    "    print(time.time() - stime)\r\n",
    "\r\n",
    "sentence = \"Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\"\r\n",
    "infer(sentence, \"output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**ZHTTS模型**\n",
    "\n",
    "因为Parakeet目前还不支持中文，后续有时间可以用中文数据集训练一下。这里暂时使用zhtts， 因为里面用到了tensorflow,所以下面的代码无法在AISTUDIO运行, 建议本地使用。\n",
    "\n",
    "zhtts在CPU上实时运行的中文语音合成系统（一个简单的示例，使用 Fastspeech2 + MbMelGan），但总体效果离“能用”还有很大差距。\n",
    "\n",
    "Github地址: https://github.com/Jackiexiao/zhtts\n",
    "\n",
    "安装方式: `!pip install zhtts --user`\n",
    "\n",
    "\n",
    "```\n",
    "import zhtts\n",
    "def is_chinese(string):\n",
    "    \"\"\"\n",
    "    检查整个字符串是否包含中文\n",
    "    :param string: 需要检查的字符串\n",
    "    :return: bool\n",
    "    \"\"\"\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "text = \"2020年，这是一个开源的端到端中文语音合成系统\"\n",
    "tts = zhtts.TTS() # use fastspeech2 by default\n",
    "tts.text2wav(text, \"demo.wav\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 七、将Wechaty接收到的带文字的图片转为语音\n",
    "\n",
    "### OCR的应用场景\n",
    "\n",
    "根据OCR的应用场景而言，我们可以大致分成识别特定场景下的专用OCR以及识别多种场景下的通用OCR。就前者而言，证件识别以及车牌识别就是专用OCR的典型案例。针对特定场景进行设计、优化以达到最好的特定场景下的效果展示。那通用的OCR就是使用在更多、更复杂的场景下，拥有比较好的泛性。在这个过程中由于场景的不确定性，比如：图片背景极其丰富、亮度不均衡、光照不均衡、残缺遮挡、文字扭曲、字体多样等等问题，会带来极大的挑战。现PaddleHub为大家提供的是超轻量级中文OCR模型，聚焦特定的场景，支持中英文数字组合式别、竖排文字识别、长文本识别场景。\n",
    "\n",
    "### OCR的技术路线\n",
    "\n",
    "典型的OCR技术路线如下图所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3dce6cab703e48139a2a9c3022ff9c55336be8179d6c452a9f3a41739eb46597)\n",
    "\n",
    "其中OCR识别的关键路径在于文字检测和文本识别部分，这也是深度学习技术可以充分发挥功效的地方。PaddleHub为大家开源的预训练模型的网络结构是Differentiable Binarization+ CRNN，基于icdar2015数据集下进行的训练。\n",
    "\n",
    "首先，DB是一种基于分割的文本检测算法。在各种文本检测算法中，基于分割的检测算法可以更好地处理弯曲等不规则形状文本，因此往往能取得更好的检测效果。但分割法后处理步骤中将分割结果转化为检测框的流程复杂，耗时严重。因此作者提出一个可微的二值化模块（Differentiable Binarization，简称DB），将二值化阈值加入训练中学习，可以获得更准确的检测边界，从而简化后处理流程。DB算法最终在5个数据集上达到了state-of-art的效果和性能。参考论文：[Real-time Scene Text Detection with Differentiable Binarization](https://arxiv.org/abs/1911.08947)\n",
    "\n",
    "下图是DB算法的结构图：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f7cb17481e84457d90f3ac4e15bf1e53e20ff53096904e36a10d048f67abddaa)\n",
    "\n",
    "\n",
    "接着，我们使用 CRNN（Convolutional Recurrent Neural Network）即卷积递归神经网络，是DCNN和RNN的组合，专门用于识别图像中的序列式对象。与CTC loss配合使用，进行文字识别，可以直接从文本词级或行级的标注中学习，不需要详细的字符级的标注。参考论文:[An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition](https://arxiv.org/pdf/1507.05717.pdf)\n",
    "\n",
    "\n",
    "下图是CRNN的网络结构图：\n",
    "\n",
    "<div align=center> <img src=\"https://ai-studio-static-online.cdn.bcebos.com/af68e45eea184b4c966f23ad7d9fd295e07e1fc31cc74134b4bd99ee275bed63\" />\n",
    "\n",
    "  \n",
    "PaddleHub继承了PaddleOCR部分文字识别模型，虽功能上不支持空格符识别，文本角度分类等， 有一定局限性，但是能满足本项目演示的需要了。最大的优势是可以支持一键预测，非常方便快捷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-09 23:15:00,386] [ WARNING] - The _initialize method in HubModule will soon be deprecated, you can use the __init__() to handle the initialization of the object\n",
      "[2021-05-09 23:15:00,787] [ WARNING] - The _initialize method in HubModule will soon be deprecated, you can use the __init__() to handle the initialization of the object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created text:  知识改变命运 学习成就未来\n",
      "created text:  Actions speak louder than words\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\r\n",
    "import cv2\r\n",
    "\r\n",
    "ocr = hub.Module(name=\"chinese_ocr_db_crnn_mobile\")\r\n",
    "test_img_path = [\"./ch.png\", \"./eng3.jpg\"]\r\n",
    "np_images =[cv2.imread(image_path) for image_path in test_img_path] \r\n",
    "\r\n",
    "results = ocr.recognize_text(\r\n",
    "                    images=np_images,         # 图片数据，ndarray.shape 为 [H, W, C]，BGR格式；\r\n",
    "                    use_gpu=False,            # 是否使用 GPU；若使用GPU，请先设置CUDA_VISIBLE_DEVICES环境变量\r\n",
    "                    output_dir='ocr_result',  # 图片的保存路径，默认设为 ocr_result；\r\n",
    "                    visualization=True,       # 是否将识别结果保存为图片文件；\r\n",
    "                    box_thresh=0.5,           # 检测文本框置信度的阈值；\r\n",
    "                    text_thresh=0.5)          # 识别中文文本置信度的阈值；\r\n",
    "\r\n",
    "for result in results:\r\n",
    "    data = result['data']\r\n",
    "    save_path = result['save_path']\r\n",
    "    txt = [d['text'] for d in data]\r\n",
    "    txt = \" \".join(txt)\r\n",
    "    print(\"created text: \",  txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 八、用语音文件生成川普附身的wav2lip视频\n",
    "\n",
    "使用上面几种不同来源的音频文件，一键生成wav2lip视频\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/linalg/__init__.py:217: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from .mio5_utils import VarReader5\n",
      "Number of frames available for inference: 1\n",
      "Extracting raw audio...\n",
      "ffmpeg version 2.8.15-0ubuntu0.16.04.1 Copyright (c) 2000-2018 the FFmpeg developers\n",
      "  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 20160609\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\n",
      "  libavutil      54. 31.100 / 54. 31.100\n",
      "  libavcodec     56. 60.100 / 56. 60.100\n",
      "  libavformat    56. 40.101 / 56. 40.101\n",
      "  libavdevice    56.  4.100 / 56.  4.100\n",
      "  libavfilter     5. 40.101 /  5. 40.101\n",
      "  libavresample   2.  1.  0 /  2.  1.  0\n",
      "  libswscale      3.  1.101 /  3.  1.101\n",
      "  libswresample   1.  2.101 /  1.  2.101\n",
      "  libpostproc    53.  3.100 / 53.  3.100\n",
      "\u001b[0;35m[mp3 @ 0xf23440] \u001b[0mSkipping 0 bytes of junk at 237.\n",
      "Input #0, mp3, from 'message-7116130031518557320-audio.mp3':\n",
      "  Metadata:\n",
      "    encoder         : Lavf56.40.101\n",
      "  Duration: 00:00:06.17, start: 0.046042, bitrate: 32 kb/s\n",
      "    Stream #0:0: Audio: mp3, 24000 Hz, mono, s16p, 32 kb/s\n",
      "Output #0, wav, to 'temp/temp.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf56.40.101\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc56.60.100 pcm_s16le\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "size=     287kB time=00:00:06.12 bitrate= 384.1kbits/s    \n",
      "video:0kB audio:287kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.026544%\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numba/types/__init__.py:110: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  long_ = _make_signed(np.long)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numba/types/__init__.py:111: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ulong = _make_unsigned(np.long)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:169: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:286: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:858: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1094: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1120: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1349: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1590: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1723: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numba/ir_utils.py:1512: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if (hasattr(numpy, value)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numba/ir_utils.py:1513: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  and def_val == getattr(numpy, value)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numba/ir_utils.py:1512: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if (hasattr(numpy, value)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numba/ir_utils.py:1513: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  and def_val == getattr(numpy, value)):\n",
      "(80, 490)\n",
      "Length of mel chunks: 150\n",
      "2021-05-07 23:54:04,965 - INFO - unique_endpoints {''}\n",
      "2021-05-07 23:54:04,965 - INFO - Found /home/aistudio/.cache/paddle/hapi/weights/wav2lip_hq.pdparams\n",
      "Model loaded\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]2021-05-07 23:54:05,289 - INFO - unique_endpoints {''}\n",
      "2021-05-07 23:54:05,289 - INFO - Found /home/aistudio/.cache/paddle/hapi/weights/s3fd_paddle.pdparams\n",
      "\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:143: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n",
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.68s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:30<00:00, 15.01s/it]\n",
      "ffmpeg version 2.8.15-0ubuntu0.16.04.1 Copyright (c) 2000-2018 the FFmpeg developers\n",
      "  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 20160609\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\n",
      "  libavutil      54. 31.100 / 54. 31.100\n",
      "  libavcodec     56. 60.100 / 56. 60.100\n",
      "  libavformat    56. 40.101 / 56. 40.101\n",
      "  libavdevice    56.  4.100 / 56.  4.100\n",
      "  libavfilter     5. 40.101 /  5. 40.101\n",
      "  libavresample   2.  1.  0 /  2.  1.  0\n",
      "  libswscale      3.  1.101 /  3.  1.101\n",
      "  libswresample   1.  2.101 /  1.  2.101\n",
      "  libpostproc    53.  3.100 / 53.  3.100\n",
      "\u001b[0;33mGuessed Channel Layout for  Input Stream #0.0 : mono\n",
      "\u001b[0mInput #0, wav, from 'temp/temp.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf56.40.101\n",
      "  Duration: 00:00:06.12, bitrate: 384 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "Input #1, avi, from 'temp/result.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.31.101\n",
      "  Duration: 00:00:06.00, start: 0.000000, bitrate: 567 kb/s\n",
      "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 558x326 [SAR 1:1 DAR 279:163], 559 kb/s, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mprofile High, level 2.1\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0m264 - core 148 r2643 5c65704 - H.264/MPEG-4 AVC codec - Copyleft 2003-2015 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=10 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'wav2lip.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf56.40.101\n",
      "    Stream #0:0: Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv420p, 558x326 [SAR 1:1 DAR 279:163], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc56.60.100 libx264\n",
      "    Stream #0:1: Audio: aac ([64][0][0][0] / 0x0040), 24000 Hz, mono, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc56.60.100 aac\n",
      "Stream mapping:\n",
      "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "frame=  150 fps=0.0 q=-1.0 Lsize=     152kB time=00:00:06.14 bitrate= 202.1kbits/s    \n",
      "video:53kB audio:94kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 3.336351%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mframe I:1     Avg QP:20.02  size: 20959\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mframe P:55    Avg QP:21.22  size:   504\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mframe B:94    Avg QP:31.32  size:    51\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mconsecutive B-frames: 16.0%  1.3%  0.0% 82.7%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mmb I  I16..4:  6.3% 91.7%  2.0%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mmb P  I16..4:  0.0%  0.2%  0.0%  P16..4:  5.5%  1.3%  1.2%  0.0%  0.0%    skip:91.8%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.5%  0.2%  0.0%  direct: 0.1%  skip:98.2%  L0:42.1% L1:54.4% BI: 3.4%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0m8x8 transform intra:91.6% inter:83.0%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mcoded y,uvDC,uvAC intra: 83.6% 86.4% 49.2% inter: 1.6% 1.7% 0.4%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mi16 v,h,dc,p: 44% 15% 38%  2%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 16% 24% 31%  5%  5%  4%  3%  5%  6%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 42% 39%  5%  1%  2%  2%  2%  3%  3%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mi8c dc,h,v,p: 31% 43% 21%  5%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mref P L0: 75.3%  8.8%  9.0%  6.8%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mref B L0: 80.2% 14.8%  5.1%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mref B L1: 89.1% 10.9%\n",
      "\u001b[1;36m[libx264 @ 0x17e4b20] \u001b[0mkb/s:71.31\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~\r\n",
    "!export PYTHONPATH=$PYTHONPATH:/home/aistudio/PaddleGAN && python PaddleGAN/applications/tools/wav2lip.py --face trump.jpg --audio message-7116130031518557320-audio.mp3 --outfile wav2lip.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 九、部署在wechaty上\n",
    "\n",
    "实测效果还可以，只是集成模型太多，导致速度变得很慢，后期需要进一步优化。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/542753aeed2044e19f5c2a1f98ec2a1d9431dca230c94bec9072d2e08b0482a8)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6b51d54251e54faa8982849b526106a8b8caacca4ebc4d93ade4eee2acf403a8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 十、总结\n",
    "\n",
    "本项目尝试了各种形式的输入来源，并转成音频文件，最终生成wav2lip视频。项目中海油诸多不足，想要工程化应用还有诸多要改善的地方。\n",
    "\n",
    "后续我将持续完善里面的逻辑，若有时间，希望将zhtts中文语音模型转到paddle中来。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**关于作者**\n",
    "\n",
    "PaddlePaddle开发爱好者\n",
    "\n",
    "我在AI Studio上获得黄金等级，点亮5个徽章，来互关呀~ https://aistudio.baidu.com/aistudio/personalcenter/thirdview/89442"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
